{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf1d5d9-ebc2-4c75-bea7-ea145ca2270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wbgapi as wb\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5c5c7-3cfd-4ef7-bdad-165a379ec8d3",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "For today's lecture, I am using the 3rd Chapter of [Introduction to Statistical Learning](https://www-bcf.usc.edu/~gareth/ISL/) by Gareth et.al., 3rd Chapter of [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) by Hastie et.al. and the 3rd Chapter of [Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) by Bishop.\n",
    "\n",
    "## Ordinary Least Squares Regression\n",
    "\n",
    "Assume we have a data set consisting of numerical pairs $(x_i,y_i)$, and assume also that upon some inspection, we guess that there is a linear dependence between $x_i$'s and $y_i$'s of the form\n",
    "\n",
    "$$  y_i \\approx \\alpha x_i + \\beta + \\epsilon $$\n",
    "\n",
    "In this scenario, we assume the $y_i$ values depend on $x_i$ values functionally, moreover, we also assume that the functional relationship is linear. In such cases, $y_i$'s are called *dependent variable* and $x_i$'s are called *independent variables*. Our task is then to calculate the best fitting $\\alpha$ and $\\beta$ for this set.\n",
    "\n",
    "## How and why does linear regression work?\n",
    "   \n",
    "The question we need to ask is, in linear regression what is our **fit criteria** or what is our **error function**?\n",
    "\n",
    "$$ RSS(\\alpha,\\beta) = \\sum_{i=1}^N (\\alpha x_i + \\beta - y_i)^2 $$\n",
    "\n",
    "The (ordinary least square) regression finds $\\alpha$ and $\\beta$ that minimizes this function $RSS(\\alpha,\\beta)$:\n",
    "$$ \\alpha = \\frac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2}, \\qquad \\beta = \\overline{y} - \\alpha \\overline{x} $$\n",
    "    \n",
    "## How do we judge if the model fit?\n",
    "\n",
    "The $R^2$-statistic tells us how much of the error is explained by the independent variable:\n",
    "$$ RSE(\\alpha,\\beta) = \\sqrt{\\frac{RSS(\\alpha,\\beta)}{n-2}}, \\qquad R^2 = 1 - \\frac{RSE}{RSS} $$\n",
    "\n",
    "\n",
    "## Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd531602-b91a-450e-b88d-6bbbf4c8bd96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>series</th>\n",
       "      <th>economy</th>\n",
       "      <th>aggregate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16487</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16488</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16489</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16490</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SE.ADT.LITR.FE.ZS</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16492 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       value             series economy  aggregate    time\n",
       "0        NaN  SE.ADT.LITR.FE.ZS     ZWE      False  YR2021\n",
       "1        NaN  SE.ADT.LITR.FE.ZS     ZWE      False  YR2020\n",
       "2        NaN  SE.ADT.LITR.FE.ZS     ZWE      False  YR2019\n",
       "3        NaN  SE.ADT.LITR.FE.ZS     ZWE      False  YR2018\n",
       "4        NaN  SE.ADT.LITR.FE.ZS     ZWE      False  YR2017\n",
       "...      ...                ...     ...        ...     ...\n",
       "16487    NaN  SE.ADT.LITR.FE.ZS     AFE       True  YR1964\n",
       "16488    NaN  SE.ADT.LITR.FE.ZS     AFE       True  YR1963\n",
       "16489    NaN  SE.ADT.LITR.FE.ZS     AFE       True  YR1962\n",
       "16490    NaN  SE.ADT.LITR.FE.ZS     AFE       True  YR1961\n",
       "16491    NaN  SE.ADT.LITR.FE.ZS     AFE       True  YR1960\n",
       "\n",
       "[16492 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litrate = pd.DataFrame(list(wb.data.fetch('SE.ADT.LITR.FE.ZS')))\n",
    "litrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f28b5c-a037-46a7-b640-e8e60e14386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>series</th>\n",
       "      <th>economy</th>\n",
       "      <th>aggregate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.9</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.2</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.8</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.0</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16487</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16488</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16489</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16490</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SH.DYN.MORT</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16492 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       value       series economy  aggregate    time\n",
       "0        NaN  SH.DYN.MORT     ZWE      False  YR2021\n",
       "1       53.9  SH.DYN.MORT     ZWE      False  YR2020\n",
       "2       54.2  SH.DYN.MORT     ZWE      False  YR2019\n",
       "3       54.8  SH.DYN.MORT     ZWE      False  YR2018\n",
       "4       57.0  SH.DYN.MORT     ZWE      False  YR2017\n",
       "...      ...          ...     ...        ...     ...\n",
       "16487    NaN  SH.DYN.MORT     AFE       True  YR1964\n",
       "16488    NaN  SH.DYN.MORT     AFE       True  YR1963\n",
       "16489    NaN  SH.DYN.MORT     AFE       True  YR1962\n",
       "16490    NaN  SH.DYN.MORT     AFE       True  YR1961\n",
       "16491    NaN  SH.DYN.MORT     AFE       True  YR1960\n",
       "\n",
       "[16492 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mortality = pd.DataFrame(list(wb.data.fetch('SH.DYN.MORT')))\n",
    "mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fd0a11-6cbe-4caa-b4f2-82c9f868397e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>literacy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YR2021</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2020</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2019</th>\n",
       "      <td>94.424042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2018</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2017</th>\n",
       "      <td>93.498268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1964</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1963</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1962</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1961</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1960</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         literacy\n",
       "time             \n",
       "YR2021        NaN\n",
       "YR2020        NaN\n",
       "YR2019  94.424042\n",
       "YR2018        NaN\n",
       "YR2017  93.498268\n",
       "...           ...\n",
       "YR1964        NaN\n",
       "YR1963        NaN\n",
       "YR1962        NaN\n",
       "YR1961        NaN\n",
       "YR1960        NaN\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltr = litrate[['time','value']][litrate['economy']=='TUR']\n",
    "ltr.index = ltr.time\n",
    "del ltr['time']\n",
    "ltr.columns = [['literacy']]\n",
    "ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56aaa058-e607-4a53-bcf5-004cb1432b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mortality</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YR2021</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2020</th>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2019</th>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2018</th>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR2017</th>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1964</th>\n",
       "      <td>225.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1963</th>\n",
       "      <td>233.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1962</th>\n",
       "      <td>241.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1961</th>\n",
       "      <td>249.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YR1960</th>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mortality\n",
       "time            \n",
       "YR2021       NaN\n",
       "YR2020       9.5\n",
       "YR2019      10.1\n",
       "YR2018      10.7\n",
       "YR2017      11.4\n",
       "...          ...\n",
       "YR1964     225.7\n",
       "YR1963     233.5\n",
       "YR1962     241.4\n",
       "YR1961     249.3\n",
       "YR1960     257.0\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr = mortality[['time','value']][mortality['economy']=='TUR']\n",
    "mtr.index = mtr.time\n",
    "del mtr['time']\n",
    "mtr.columns = [['mortality']]\n",
    "mtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4938fa-9f64-4253-84a6-1c553c79eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(df,cntry,name):\n",
    "    tmp = df[['time','value']][df['economy']==cntry]\n",
    "    tmp.index = tmp.time\n",
    "    del tmp['time']\n",
    "    tmp.columns = [[name]]\n",
    "    return tmp\n",
    "\n",
    "def litvsmor(cntry):\n",
    "    lit = extract(litrate,cntry,'literacy')\n",
    "    mor = extract(mortality,cntry,'mortality')\n",
    "    res = mor.join(lit)\n",
    "    res.dropna(inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1ce9e7-ae09-4722-9c91-dec8b734f428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6cab537490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr = litvsmor('TUR')\n",
    "plt.scatter(tr['literacy'],tr['mortality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db80dd3-404d-44c2-b6ab-676df5e9a270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              mortality   R-squared:                       0.945\n",
      "Model:                            OLS   Adj. R-squared:                  0.942\n",
      "Method:                 Least Squares   F-statistic:                     275.9\n",
      "Date:                Sun, 16 Oct 2022   Prob (F-statistic):           1.64e-11\n",
      "Time:                        19:52:12   Log-Likelihood:                -67.223\n",
      "No. Observations:                  18   AIC:                             138.4\n",
      "Df Residuals:                      16   BIC:                             140.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const           273.0338     14.304     19.089      0.000     242.712     303.356\n",
      "('literacy',)    -2.8762      0.173    -16.611      0.000      -3.243      -2.509\n",
      "==============================================================================\n",
      "Omnibus:                        2.728   Durbin-Watson:                   0.508\n",
      "Prob(Omnibus):                  0.256   Jarque-Bera (JB):                1.852\n",
      "Skew:                          -0.596   Prob(JB):                        0.396\n",
      "Kurtosis:                       1.976   Cond. No.                         467.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1477: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=18\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6cab24ae30>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = litvsmor('TUR')\n",
    "X = res['literacy']\n",
    "Y = res['mortality']\n",
    "XX = sm.add_constant(X)\n",
    "model = sm.OLS(Y,XX)\n",
    "results = model.fit()\n",
    "alpha,beta = results.params\n",
    "print(results.summary())\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.plot(X,alpha + beta*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a976e7b-247e-42ee-9014-32b2863a90d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>series</th>\n",
       "      <th>economy</th>\n",
       "      <th>aggregate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.3</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.3</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>False</td>\n",
       "      <td>YR2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16487</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16488</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16489</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16490</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SI.POV.GINI</td>\n",
       "      <td>AFE</td>\n",
       "      <td>True</td>\n",
       "      <td>YR1960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16492 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       value       series economy  aggregate    time\n",
       "0        NaN  SI.POV.GINI     ZWE      False  YR2021\n",
       "1        NaN  SI.POV.GINI     ZWE      False  YR2020\n",
       "2       50.3  SI.POV.GINI     ZWE      False  YR2019\n",
       "3        NaN  SI.POV.GINI     ZWE      False  YR2018\n",
       "4       44.3  SI.POV.GINI     ZWE      False  YR2017\n",
       "...      ...          ...     ...        ...     ...\n",
       "16487    NaN  SI.POV.GINI     AFE       True  YR1964\n",
       "16488    NaN  SI.POV.GINI     AFE       True  YR1963\n",
       "16489    NaN  SI.POV.GINI     AFE       True  YR1962\n",
       "16490    NaN  SI.POV.GINI     AFE       True  YR1961\n",
       "16491    NaN  SI.POV.GINI     AFE       True  YR1960\n",
       "\n",
       "[16492 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini = pd.DataFrame(list(wb.data.fetch('SI.POV.GINI')))\n",
    "gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c8c1f5-abd6-4ca8-92d3-599c6c00203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morvsgini(cntry):\n",
    "    gin = extract(gini,cntry,'gini')\n",
    "    mor = extract(mortality,cntry,'mortality')\n",
    "    res = gin.join(mor)\n",
    "    res.dropna(inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d16130-1432-42ba-8271-a8a9445f5e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              mortality   R-squared:                       0.117\n",
      "Model:                            OLS   Adj. R-squared:                  0.068\n",
      "Method:                 Least Squares   F-statistic:                     2.390\n",
      "Date:                Sun, 16 Oct 2022   Prob (F-statistic):              0.140\n",
      "Time:                        19:52:40   Log-Likelihood:                -85.053\n",
      "No. Observations:                  20   AIC:                             174.1\n",
      "Df Residuals:                      18   BIC:                             176.1\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -153.7370    115.269     -1.334      0.199    -395.908      88.434\n",
      "('gini',)      4.3506      2.814      1.546      0.140      -1.562      10.263\n",
      "==============================================================================\n",
      "Omnibus:                       14.299   Durbin-Watson:                   0.259\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               12.989\n",
      "Skew:                           1.519   Prob(JB):                      0.00151\n",
      "Kurtosis:                       5.521   Cond. No.                     1.18e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.18e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6caaeac040>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = morvsgini('TUR')\n",
    "X = res['gini']\n",
    "Y = res['mortality']\n",
    "XX = sm.add_constant(X)\n",
    "model = sm.OLS(Y,XX)\n",
    "results = model.fit()\n",
    "alpha,beta = results.params\n",
    "print(results.summary())\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.plot(X,alpha + beta*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f44f21-15ba-485b-a7ee-51c890efc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def litvsgini(cntry):\n",
    "    lit = extract(litrate,cntry,'literacy')\n",
    "    gin = extract(gini,cntry,'gini')\n",
    "    res = gin.join(lit)\n",
    "    res.dropna(inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3434a5-1a3e-460c-9323-895698a59a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6caacf3700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = litvsgini('PAK')\n",
    "plt.scatter(res['literacy'],res['gini'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0148fbc-3578-4521-b757-b913f0bc65e5",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "This time the data set we have is of the form $(x_{1i},\\ldots,x_{mi},y_i)$ and we assume we have a functional relation of the form\n",
    "$$ y_i \\approx \\beta + \\sum_j \\alpha_j x_{ji} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e06bee-9218-4a51-ab32-c9c08e07956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit_gin_mor(cntry):\n",
    "    lit = extract(litrate,cntry,'literacy')\n",
    "    gin = extract(gini,cntry,'gini')\n",
    "    mor = extract(mortality,cntry,'mortality')\n",
    "    res = lit.join([gin,mor])\n",
    "    res.dropna(inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa573d98-5fe3-4035-8d6b-4292ee667864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              mortality   R-squared:                       0.973\n",
      "Model:                            OLS   Adj. R-squared:                  0.966\n",
      "Method:                 Least Squares   F-statistic:                     159.6\n",
      "Date:                Sun, 16 Oct 2022   Prob (F-statistic):           9.36e-08\n",
      "Time:                        19:52:41   Log-Likelihood:                -21.734\n",
      "No. Observations:                  12   AIC:                             49.47\n",
      "Df Residuals:                       9   BIC:                             50.92\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const           554.8135     31.519     17.603      0.000     483.513     626.114\n",
      "('literacy',)    -5.3746      0.302    -17.770      0.000      -6.059      -4.690\n",
      "('gini',)        -0.2317      0.145     -1.601      0.144      -0.559       0.096\n",
      "==============================================================================\n",
      "Omnibus:                        0.659   Durbin-Watson:                   0.588\n",
      "Prob(Omnibus):                  0.719   Jarque-Bera (JB):                0.500\n",
      "Skew:                          -0.428   Prob(JB):                        0.779\n",
      "Kurtosis:                       2.482   Cond. No.                     6.86e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.86e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1477: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=12\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    }
   ],
   "source": [
    "res = lit_gin_mor('ARG')\n",
    "X = res[['literacy','gini']]\n",
    "XX = sm.add_constant(X)\n",
    "Y = res['mortality']\n",
    "model = sm.OLS(Y,XX)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d0aa36-3ea8-4419-b69f-2ffff124dd47",
   "metadata": {},
   "source": [
    "## Which of the independent variables explain the dependent variable better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b0845e-7b13-452b-99e6-59b44b63d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef839240-bb66-4329-a291-70d0196db307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              mortality   R-squared:                       0.980\n",
      "Model:                            OLS   Adj. R-squared:                  0.973\n",
      "Method:                 Least Squares   F-statistic:                     132.8\n",
      "Date:                Sun, 16 Oct 2022   Prob (F-statistic):           3.66e-07\n",
      "Time:                        19:52:41   Log-Likelihood:                -19.745\n",
      "No. Observations:                  12   AIC:                             47.49\n",
      "Df Residuals:                       8   BIC:                             49.43\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept      1601.5126    590.918      2.710      0.027     238.854    2964.171\n",
      "gini            -25.2245     14.094     -1.790      0.111     -57.726       7.277\n",
      "literacy        -16.1502      6.082     -2.655      0.029     -30.176      -2.124\n",
      "gini:literacy     0.2573      0.145      1.773      0.114      -0.077       0.592\n",
      "==============================================================================\n",
      "Omnibus:                        1.210   Durbin-Watson:                   1.017\n",
      "Prob(Omnibus):                  0.546   Jarque-Bera (JB):                0.918\n",
      "Skew:                          -0.591   Prob(JB):                        0.632\n",
      "Kurtosis:                       2.338   Cond. No.                     5.73e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.73e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1477: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=12\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gini</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.137226</td>\n",
       "      <td>10.137226</td>\n",
       "      <td>4.296553</td>\n",
       "      <td>7.191405e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>literacy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>922.530463</td>\n",
       "      <td>922.530463</td>\n",
       "      <td>391.004525</td>\n",
       "      <td>4.454714e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gini:literacy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.419726</td>\n",
       "      <td>7.419726</td>\n",
       "      <td>3.144770</td>\n",
       "      <td>1.141019e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>8.0</td>\n",
       "      <td>18.875085</td>\n",
       "      <td>2.359386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                df      sum_sq     mean_sq           F        PR(>F)\n",
       "gini           1.0   10.137226   10.137226    4.296553  7.191405e-02\n",
       "literacy       1.0  922.530463  922.530463  391.004525  4.454714e-08\n",
       "gini:literacy  1.0    7.419726    7.419726    3.144770  1.141019e-01\n",
       "Residual       8.0   18.875085    2.359386         NaN           NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ols('mortality ~ gini * literacy', data=res).fit()\n",
    "print(model.summary())\n",
    "sm.stats.anova_lm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171e79a-4199-4e7e-9100-e93cfc873b9c",
   "metadata": {},
   "source": [
    "## Another example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f74f35d-7065-4c8d-9f8c-a1704b93fa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pt</th>\n",
       "      <th>BP</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>BSA</th>\n",
       "      <th>Dur</th>\n",
       "      <th>Pulse</th>\n",
       "      <th>Stress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>47</td>\n",
       "      <td>85.4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>5.1</td>\n",
       "      <td>63</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>49</td>\n",
       "      <td>94.2</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.8</td>\n",
       "      <td>70</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>116</td>\n",
       "      <td>49</td>\n",
       "      <td>95.3</td>\n",
       "      <td>1.98</td>\n",
       "      <td>8.2</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>50</td>\n",
       "      <td>94.7</td>\n",
       "      <td>2.01</td>\n",
       "      <td>5.8</td>\n",
       "      <td>73</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>112</td>\n",
       "      <td>51</td>\n",
       "      <td>89.4</td>\n",
       "      <td>1.89</td>\n",
       "      <td>7.0</td>\n",
       "      <td>72</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "      <td>48</td>\n",
       "      <td>99.5</td>\n",
       "      <td>2.25</td>\n",
       "      <td>9.3</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>121</td>\n",
       "      <td>49</td>\n",
       "      <td>99.8</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.5</td>\n",
       "      <td>69</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>47</td>\n",
       "      <td>90.9</td>\n",
       "      <td>1.90</td>\n",
       "      <td>6.2</td>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>110</td>\n",
       "      <td>49</td>\n",
       "      <td>89.2</td>\n",
       "      <td>1.83</td>\n",
       "      <td>7.1</td>\n",
       "      <td>69</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>114</td>\n",
       "      <td>48</td>\n",
       "      <td>92.7</td>\n",
       "      <td>2.07</td>\n",
       "      <td>5.6</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>114</td>\n",
       "      <td>47</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2.07</td>\n",
       "      <td>5.3</td>\n",
       "      <td>74</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>115</td>\n",
       "      <td>49</td>\n",
       "      <td>94.1</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.6</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>114</td>\n",
       "      <td>50</td>\n",
       "      <td>91.6</td>\n",
       "      <td>2.05</td>\n",
       "      <td>10.2</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>106</td>\n",
       "      <td>45</td>\n",
       "      <td>87.1</td>\n",
       "      <td>1.92</td>\n",
       "      <td>5.6</td>\n",
       "      <td>67</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>125</td>\n",
       "      <td>52</td>\n",
       "      <td>101.3</td>\n",
       "      <td>2.19</td>\n",
       "      <td>10.0</td>\n",
       "      <td>76</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>114</td>\n",
       "      <td>46</td>\n",
       "      <td>94.5</td>\n",
       "      <td>1.98</td>\n",
       "      <td>7.4</td>\n",
       "      <td>69</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>106</td>\n",
       "      <td>46</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.6</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>113</td>\n",
       "      <td>46</td>\n",
       "      <td>94.5</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.3</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>110</td>\n",
       "      <td>48</td>\n",
       "      <td>90.5</td>\n",
       "      <td>1.88</td>\n",
       "      <td>9.0</td>\n",
       "      <td>71</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>122</td>\n",
       "      <td>56</td>\n",
       "      <td>95.7</td>\n",
       "      <td>2.09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pt   BP  Age  Weight   BSA   Dur  Pulse  Stress\n",
       "0    1  105   47    85.4  1.75   5.1     63      33\n",
       "1    2  115   49    94.2  2.10   3.8     70      14\n",
       "2    3  116   49    95.3  1.98   8.2     72      10\n",
       "3    4  117   50    94.7  2.01   5.8     73      99\n",
       "4    5  112   51    89.4  1.89   7.0     72      95\n",
       "5    6  121   48    99.5  2.25   9.3     71      10\n",
       "6    7  121   49    99.8  2.25   2.5     69      42\n",
       "7    8  110   47    90.9  1.90   6.2     66       8\n",
       "8    9  110   49    89.2  1.83   7.1     69      62\n",
       "9   10  114   48    92.7  2.07   5.6     64      35\n",
       "10  11  114   47    94.4  2.07   5.3     74      90\n",
       "11  12  115   49    94.1  1.98   5.6     71      21\n",
       "12  13  114   50    91.6  2.05  10.2     68      47\n",
       "13  14  106   45    87.1  1.92   5.6     67      80\n",
       "14  15  125   52   101.3  2.19  10.0     76      98\n",
       "15  16  114   46    94.5  1.98   7.4     69      95\n",
       "16  17  106   46    87.0  1.87   3.6     62      18\n",
       "17  18  113   46    94.5  1.90   4.3     70      12\n",
       "18  19  110   48    90.5  1.88   9.0     71      99\n",
       "19  20  122   56    95.7  2.09   7.0     75      99"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp = pd.read_csv('https://online.stat.psu.edu/onlinecourses/sites/stat501/files/data/bloodpress.txt',sep='\\t')\n",
    "bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6295f47-31e8-43c0-96fe-241fcb3794e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Stress</td>      <th>  R-squared:         </th> <td>   0.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>0.0776</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:52:43</td>     <th>  Log-Likelihood:    </th> <td> -79.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    20</td>      <th>  AIC:               </th> <td>   186.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     6</td>      <th>  BIC:               </th> <td>   200.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>-9007.9229</td> <td> 1.25e+04</td> <td>   -0.721</td> <td> 0.498</td> <td>-3.96e+04</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BP</th>               <td>   52.9171</td> <td>   17.263</td> <td>    3.065</td> <td> 0.022</td> <td>   10.677</td> <td>   95.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BSA</th>              <td>  -21.6036</td> <td>  118.091</td> <td>   -0.183</td> <td> 0.861</td> <td> -310.561</td> <td>  267.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dur</th>              <td> 2253.2385</td> <td> 1513.233</td> <td>    1.489</td> <td> 0.187</td> <td>-1449.508</td> <td> 5955.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pulse</th>            <td>  117.4199</td> <td>  184.164</td> <td>    0.638</td> <td> 0.547</td> <td> -333.213</td> <td>  568.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dur:Pulse</th>        <td>  -26.3437</td> <td>   21.714</td> <td>   -1.213</td> <td> 0.271</td> <td>  -79.476</td> <td>   26.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Age</th>              <td>  663.6003</td> <td>  619.918</td> <td>    1.070</td> <td> 0.326</td> <td> -853.285</td> <td> 2180.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weight</th>           <td> -306.0697</td> <td>  251.173</td> <td>   -1.219</td> <td> 0.269</td> <td> -920.669</td> <td>  308.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dur:Age</th>          <td> -117.1711</td> <td>   90.464</td> <td>   -1.295</td> <td> 0.243</td> <td> -338.528</td> <td>  104.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dur:Weight</th>       <td>   34.3685</td> <td>   40.635</td> <td>    0.846</td> <td> 0.430</td> <td>  -65.062</td> <td>  133.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pulse:Age</th>        <td>  -10.0396</td> <td>    8.985</td> <td>   -1.117</td> <td> 0.307</td> <td>  -32.024</td> <td>   11.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pulse:Weight</th>     <td>    3.8660</td> <td>    3.583</td> <td>    1.079</td> <td> 0.322</td> <td>   -4.900</td> <td>   12.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dur:Pulse:Age</th>    <td>    1.6601</td> <td>    1.308</td> <td>    1.269</td> <td> 0.252</td> <td>   -1.542</td> <td>    4.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dur:Pulse:Weight</th> <td>   -0.5453</td> <td>    0.584</td> <td>   -0.935</td> <td> 0.386</td> <td>   -1.973</td> <td>    0.883</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.890</td> <th>  Durbin-Watson:     </th> <td>   2.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.641</td> <th>  Jarque-Bera (JB):  </th> <td>   0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.424</td> <th>  Prob(JB):          </th> <td>   0.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.493</td> <th>  Cond. No.          </th> <td>1.24e+08</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.24e+08. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Stress   R-squared:                       0.876\n",
       "Model:                            OLS   Adj. R-squared:                  0.607\n",
       "Method:                 Least Squares   F-statistic:                     3.262\n",
       "Date:                Sun, 16 Oct 2022   Prob (F-statistic):             0.0776\n",
       "Time:                        19:52:43   Log-Likelihood:                -79.253\n",
       "No. Observations:                  20   AIC:                             186.5\n",
       "Df Residuals:                       6   BIC:                             200.4\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept        -9007.9229   1.25e+04     -0.721      0.498   -3.96e+04    2.16e+04\n",
       "BP                  52.9171     17.263      3.065      0.022      10.677      95.158\n",
       "BSA                -21.6036    118.091     -0.183      0.861    -310.561     267.354\n",
       "Dur               2253.2385   1513.233      1.489      0.187   -1449.508    5955.985\n",
       "Pulse              117.4199    184.164      0.638      0.547    -333.213     568.053\n",
       "Dur:Pulse          -26.3437     21.714     -1.213      0.271     -79.476      26.788\n",
       "Age                663.6003    619.918      1.070      0.326    -853.285    2180.486\n",
       "Weight            -306.0697    251.173     -1.219      0.269    -920.669     308.529\n",
       "Dur:Age           -117.1711     90.464     -1.295      0.243    -338.528     104.186\n",
       "Dur:Weight          34.3685     40.635      0.846      0.430     -65.062     133.799\n",
       "Pulse:Age          -10.0396      8.985     -1.117      0.307     -32.024      11.945\n",
       "Pulse:Weight         3.8660      3.583      1.079      0.322      -4.900      12.632\n",
       "Dur:Pulse:Age        1.6601      1.308      1.269      0.252      -1.542       4.862\n",
       "Dur:Pulse:Weight    -0.5453      0.584     -0.935      0.386      -1.973       0.883\n",
       "==============================================================================\n",
       "Omnibus:                        0.890   Durbin-Watson:                   2.616\n",
       "Prob(Omnibus):                  0.641   Jarque-Bera (JB):                0.814\n",
       "Skew:                          -0.424   Prob(JB):                        0.666\n",
       "Kurtosis:                       2.493   Cond. No.                     1.24e+08\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.24e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ols('Stress ~ BP + BSA + Dur*Pulse*(Age+Weight)', data = bp).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d669eb94-5523-415c-9714-0af5f7f909d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BP</th>\n",
       "      <td>1.0</td>\n",
       "      <td>702.016071</td>\n",
       "      <td>702.016071</td>\n",
       "      <td>1.300260</td>\n",
       "      <td>0.297629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSA</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1591.992348</td>\n",
       "      <td>1591.992348</td>\n",
       "      <td>2.948657</td>\n",
       "      <td>0.136759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dur</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1260.688640</td>\n",
       "      <td>1260.688640</td>\n",
       "      <td>2.335023</td>\n",
       "      <td>0.177355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulse</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5753.893913</td>\n",
       "      <td>5753.893913</td>\n",
       "      <td>10.657249</td>\n",
       "      <td>0.017151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dur:Pulse</th>\n",
       "      <td>1.0</td>\n",
       "      <td>432.215881</td>\n",
       "      <td>432.215881</td>\n",
       "      <td>0.800542</td>\n",
       "      <td>0.405391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1247.249270</td>\n",
       "      <td>1247.249270</td>\n",
       "      <td>2.310130</td>\n",
       "      <td>0.179344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3729.689370</td>\n",
       "      <td>3729.689370</td>\n",
       "      <td>6.908057</td>\n",
       "      <td>0.039148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dur:Age</th>\n",
       "      <td>1.0</td>\n",
       "      <td>339.219556</td>\n",
       "      <td>339.219556</td>\n",
       "      <td>0.628296</td>\n",
       "      <td>0.458168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dur:Weight</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5195.040263</td>\n",
       "      <td>5195.040263</td>\n",
       "      <td>9.622151</td>\n",
       "      <td>0.021064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulse:Age</th>\n",
       "      <td>1.0</td>\n",
       "      <td>806.614693</td>\n",
       "      <td>806.614693</td>\n",
       "      <td>1.493996</td>\n",
       "      <td>0.267431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulse:Weight</th>\n",
       "      <td>1.0</td>\n",
       "      <td>625.970804</td>\n",
       "      <td>625.970804</td>\n",
       "      <td>1.159411</td>\n",
       "      <td>0.322965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dur:Pulse:Age</th>\n",
       "      <td>1.0</td>\n",
       "      <td>736.988017</td>\n",
       "      <td>736.988017</td>\n",
       "      <td>1.365035</td>\n",
       "      <td>0.286984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dur:Pulse:Weight</th>\n",
       "      <td>1.0</td>\n",
       "      <td>471.545640</td>\n",
       "      <td>471.545640</td>\n",
       "      <td>0.873388</td>\n",
       "      <td>0.386087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3239.425533</td>\n",
       "      <td>539.904256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   df       sum_sq      mean_sq          F    PR(>F)\n",
       "BP                1.0   702.016071   702.016071   1.300260  0.297629\n",
       "BSA               1.0  1591.992348  1591.992348   2.948657  0.136759\n",
       "Dur               1.0  1260.688640  1260.688640   2.335023  0.177355\n",
       "Pulse             1.0  5753.893913  5753.893913  10.657249  0.017151\n",
       "Dur:Pulse         1.0   432.215881   432.215881   0.800542  0.405391\n",
       "Age               1.0  1247.249270  1247.249270   2.310130  0.179344\n",
       "Weight            1.0  3729.689370  3729.689370   6.908057  0.039148\n",
       "Dur:Age           1.0   339.219556   339.219556   0.628296  0.458168\n",
       "Dur:Weight        1.0  5195.040263  5195.040263   9.622151  0.021064\n",
       "Pulse:Age         1.0   806.614693   806.614693   1.493996  0.267431\n",
       "Pulse:Weight      1.0   625.970804   625.970804   1.159411  0.322965\n",
       "Dur:Pulse:Age     1.0   736.988017   736.988017   1.365035  0.286984\n",
       "Dur:Pulse:Weight  1.0   471.545640   471.545640   0.873388  0.386087\n",
       "Residual          6.0  3239.425533   539.904256        NaN       NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.stats.anova_lm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9d71c53-a973-4a37-9d5a-e803cb54fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51271bf2-6977-437a-a144-19fb5579a61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "ms = yf.download('MSFT')\n",
    "ap = yf.download('AAPL')\n",
    "cs = yf.download('CSCO')\n",
    "nd = yf.download('NDX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0a6eeee-535d-470c-b607-ffde0342c196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ms</th>\n",
       "      <th>ap</th>\n",
       "      <th>cs</th>\n",
       "      <th>nd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-02-16</th>\n",
       "      <td>0.682292</td>\n",
       "      <td>0.305804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>209.929993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-20</th>\n",
       "      <td>0.678819</td>\n",
       "      <td>0.299107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>208.354996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-21</th>\n",
       "      <td>0.685764</td>\n",
       "      <td>0.292411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>205.065002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-22</th>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>204.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-23</th>\n",
       "      <td>0.664931</td>\n",
       "      <td>0.292411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>204.190002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-10</th>\n",
       "      <td>233.050003</td>\n",
       "      <td>140.419998</td>\n",
       "      <td>40.630001</td>\n",
       "      <td>11048.509766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-11</th>\n",
       "      <td>227.619995</td>\n",
       "      <td>139.899994</td>\n",
       "      <td>39.810001</td>\n",
       "      <td>10865.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-12</th>\n",
       "      <td>225.399994</td>\n",
       "      <td>139.130005</td>\n",
       "      <td>39.700001</td>\n",
       "      <td>10810.299805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-13</th>\n",
       "      <td>219.850006</td>\n",
       "      <td>134.990005</td>\n",
       "      <td>39.029999</td>\n",
       "      <td>10481.580078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-14</th>\n",
       "      <td>235.539993</td>\n",
       "      <td>144.309998</td>\n",
       "      <td>40.840000</td>\n",
       "      <td>11130.080078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8229 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ms          ap         cs            nd\n",
       "Date                                                       \n",
       "1990-02-16    0.682292    0.305804   0.000000    209.929993\n",
       "1990-02-20    0.678819    0.299107   0.000000    208.354996\n",
       "1990-02-21    0.685764    0.292411   0.000000    205.065002\n",
       "1990-02-22    0.671875    0.303571   0.000000    204.750000\n",
       "1990-02-23    0.664931    0.292411   0.000000    204.190002\n",
       "...                ...         ...        ...           ...\n",
       "2022-10-10  233.050003  140.419998  40.630001  11048.509766\n",
       "2022-10-11  227.619995  139.899994  39.810001  10865.320312\n",
       "2022-10-12  225.399994  139.130005  39.700001  10810.299805\n",
       "2022-10-13  219.850006  134.990005  39.029999  10481.580078\n",
       "2022-10-14  235.539993  144.309998  40.840000  11130.080078\n",
       "\n",
       "[8229 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = {}\n",
    "tmp['ms'] = ms['Open']\n",
    "tmp['ap'] = ap['Open']\n",
    "tmp['cs'] = cs['Open']\n",
    "tmp['nd'] = nd['Open']\n",
    "data = pd.DataFrame(tmp).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04c98375-8d25-4ed3-9e97-2cc6694e274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>ms</td>        <th>  R-squared:         </th> <td>   0.972</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.972</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>9.674e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:41:59</td>     <th>  Log-Likelihood:    </th> <td> -31540.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  8229</td>      <th>  AIC:               </th> <td>6.309e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  8225</td>      <th>  BIC:               </th> <td>6.312e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.1722</td> <td>    0.229</td> <td>    5.111</td> <td> 0.000</td> <td>    0.723</td> <td>    1.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ap</th>        <td>    1.2465</td> <td>    0.021</td> <td>   60.505</td> <td> 0.000</td> <td>    1.206</td> <td>    1.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cs</th>        <td>    0.4508</td> <td>    0.020</td> <td>   22.380</td> <td> 0.000</td> <td>    0.411</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nd</th>        <td>    0.0043</td> <td>    0.000</td> <td>   15.429</td> <td> 0.000</td> <td>    0.004</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>70.127</td> <th>  Durbin-Watson:     </th> <td>   0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 107.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.059</td> <th>  Prob(JB):          </th> <td>4.31e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.547</td> <th>  Cond. No.          </th> <td>8.76e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.76e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     ms   R-squared:                       0.972\n",
       "Model:                            OLS   Adj. R-squared:                  0.972\n",
       "Method:                 Least Squares   F-statistic:                 9.674e+04\n",
       "Date:                Sun, 16 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        20:41:59   Log-Likelihood:                -31540.\n",
       "No. Observations:                8229   AIC:                         6.309e+04\n",
       "Df Residuals:                    8225   BIC:                         6.312e+04\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.1722      0.229      5.111      0.000       0.723       1.622\n",
       "ap             1.2465      0.021     60.505      0.000       1.206       1.287\n",
       "cs             0.4508      0.020     22.380      0.000       0.411       0.490\n",
       "nd             0.0043      0.000     15.429      0.000       0.004       0.005\n",
       "==============================================================================\n",
       "Omnibus:                       70.127   Durbin-Watson:                   0.008\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              107.602\n",
       "Skew:                           0.059   Prob(JB):                     4.31e-24\n",
       "Kurtosis:                       3.547   Cond. No.                     8.76e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.76e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ols('ms ~ ap + cs + nd', data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2913e0ec-fd13-4486-9750-f32a91c341f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ap</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.556548e+07</td>\n",
       "      <td>3.556548e+07</td>\n",
       "      <td>284563.565977</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.780195e+05</td>\n",
       "      <td>6.780195e+05</td>\n",
       "      <td>5424.913114</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nd</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.975188e+04</td>\n",
       "      <td>2.975188e+04</td>\n",
       "      <td>238.048294</td>\n",
       "      <td>5.759128e-53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>8225.0</td>\n",
       "      <td>1.027981e+06</td>\n",
       "      <td>1.249825e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              df        sum_sq       mean_sq              F        PR(>F)\n",
       "ap           1.0  3.556548e+07  3.556548e+07  284563.565977  0.000000e+00\n",
       "cs           1.0  6.780195e+05  6.780195e+05    5424.913114  0.000000e+00\n",
       "nd           1.0  2.975188e+04  2.975188e+04     238.048294  5.759128e-53\n",
       "Residual  8225.0  1.027981e+06  1.249825e+02            NaN           NaN"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.stats.anova_lm(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ce277-1516-430c-af88-7be1578ffa78",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31e06dbf-0cad-4420-b645-5ad562ca2f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  target  \n",
       "0        -122.23   4.526  \n",
       "1        -122.22   3.585  \n",
       "2        -122.24   3.521  \n",
       "3        -122.25   3.413  \n",
       "4        -122.25   3.422  \n",
       "...          ...     ...  \n",
       "20635    -121.09   0.781  \n",
       "20636    -121.21   0.771  \n",
       "20637    -121.22   0.923  \n",
       "20638    -121.32   0.847  \n",
       "20639    -121.24   0.894  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "data['target'] = target\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3294a338-2c82-43b6-8b18-a977d7d3e527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared:         </th> <td>   0.606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3970.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:52:44</td>     <th>  Log-Likelihood:    </th> <td> -22624.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 20640</td>      <th>  AIC:               </th> <td>4.527e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 20631</td>      <th>  BIC:               </th> <td>4.534e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>  -36.9419</td> <td>    0.659</td> <td>  -56.067</td> <td> 0.000</td> <td>  -38.233</td> <td>  -35.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HouseAge</th>   <td>    0.0094</td> <td>    0.000</td> <td>   21.143</td> <td> 0.000</td> <td>    0.009</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveRooms</th>   <td>   -0.1073</td> <td>    0.006</td> <td>  -18.235</td> <td> 0.000</td> <td>   -0.119</td> <td>   -0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Population</th> <td>-3.976e-06</td> <td> 4.75e-06</td> <td>   -0.837</td> <td> 0.402</td> <td>-1.33e-05</td> <td> 5.33e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveOccup</th>   <td>   -0.0038</td> <td>    0.000</td> <td>   -7.769</td> <td> 0.000</td> <td>   -0.005</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Longitude</th>  <td>   -0.4345</td> <td>    0.008</td> <td>  -57.682</td> <td> 0.000</td> <td>   -0.449</td> <td>   -0.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MedInc</th>     <td>    0.4367</td> <td>    0.004</td> <td>  104.054</td> <td> 0.000</td> <td>    0.428</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveBedrms</th>  <td>    0.6451</td> <td>    0.028</td> <td>   22.928</td> <td> 0.000</td> <td>    0.590</td> <td>    0.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Latitude</th>   <td>   -0.4213</td> <td>    0.007</td> <td>  -58.541</td> <td> 0.000</td> <td>   -0.435</td> <td>   -0.407</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4393.650</td> <th>  Durbin-Watson:     </th> <td>   0.885</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>14087.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.082</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 6.420</td>  <th>  Cond. No.          </th> <td>2.38e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.38e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   R-squared:                       0.606\n",
       "Model:                            OLS   Adj. R-squared:                  0.606\n",
       "Method:                 Least Squares   F-statistic:                     3970.\n",
       "Date:                Sun, 16 Oct 2022   Prob (F-statistic):               0.00\n",
       "Time:                        19:52:44   Log-Likelihood:                -22624.\n",
       "No. Observations:               20640   AIC:                         4.527e+04\n",
       "Df Residuals:                   20631   BIC:                         4.534e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -36.9419      0.659    -56.067      0.000     -38.233     -35.650\n",
       "HouseAge       0.0094      0.000     21.143      0.000       0.009       0.010\n",
       "AveRooms      -0.1073      0.006    -18.235      0.000      -0.119      -0.096\n",
       "Population -3.976e-06   4.75e-06     -0.837      0.402   -1.33e-05    5.33e-06\n",
       "AveOccup      -0.0038      0.000     -7.769      0.000      -0.005      -0.003\n",
       "Longitude     -0.4345      0.008    -57.682      0.000      -0.449      -0.420\n",
       "MedInc         0.4367      0.004    104.054      0.000       0.428       0.445\n",
       "AveBedrms      0.6451      0.028     22.928      0.000       0.590       0.700\n",
       "Latitude      -0.4213      0.007    -58.541      0.000      -0.435      -0.407\n",
       "==============================================================================\n",
       "Omnibus:                     4393.650   Durbin-Watson:                   0.885\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14087.596\n",
       "Skew:                           1.082   Prob(JB):                         0.00\n",
       "Kurtosis:                       6.420   Cond. No.                     2.38e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.38e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ols('target ~  HouseAge + AveRooms +  Population + AveOccup + Longitude + MedInc + AveBedrms + Latitude', data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d97b9f6-5d6a-4273-ae1e-ac5860d6c970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>1.0</td>\n",
       "      <td>306.610949</td>\n",
       "      <td>306.610949</td>\n",
       "      <td>584.522192</td>\n",
       "      <td>2.297689e-127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>1.0</td>\n",
       "      <td>795.653469</td>\n",
       "      <td>795.653469</td>\n",
       "      <td>1516.831383</td>\n",
       "      <td>3.136329e-320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22.132566</td>\n",
       "      <td>22.132566</td>\n",
       "      <td>42.193457</td>\n",
       "      <td>8.456212e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.925082</td>\n",
       "      <td>19.925082</td>\n",
       "      <td>37.985116</td>\n",
       "      <td>7.260819e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.288079</td>\n",
       "      <td>23.288079</td>\n",
       "      <td>44.396324</td>\n",
       "      <td>2.749560e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12979.269020</td>\n",
       "      <td>12979.269020</td>\n",
       "      <td>24743.639481</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>1.0</td>\n",
       "      <td>716.654012</td>\n",
       "      <td>716.654012</td>\n",
       "      <td>1366.227056</td>\n",
       "      <td>1.216909e-289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1797.679863</td>\n",
       "      <td>1797.679863</td>\n",
       "      <td>3427.091492</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>20631.0</td>\n",
       "      <td>10821.985155</td>\n",
       "      <td>0.524550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 df        sum_sq       mean_sq             F         PR(>F)\n",
       "HouseAge        1.0    306.610949    306.610949    584.522192  2.297689e-127\n",
       "AveRooms        1.0    795.653469    795.653469   1516.831383  3.136329e-320\n",
       "Population      1.0     22.132566     22.132566     42.193457   8.456212e-11\n",
       "AveOccup        1.0     19.925082     19.925082     37.985116   7.260819e-10\n",
       "Longitude       1.0     23.288079     23.288079     44.396324   2.749560e-11\n",
       "MedInc          1.0  12979.269020  12979.269020  24743.639481   0.000000e+00\n",
       "AveBedrms       1.0    716.654012    716.654012   1366.227056  1.216909e-289\n",
       "Latitude        1.0   1797.679863   1797.679863   3427.091492   0.000000e+00\n",
       "Residual    20631.0  10821.985155      0.524550           NaN            NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.stats.anova_lm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c7a0f45-83d1-4927-9a15-c9cd29c23568",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b84fa162-f1de-4a87-acbe-4330303e9de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MedInc': 0.44, 'HouseAge': 0.01, 'AveRooms': -0.11, 'AveBedrms': 0.65, 'Population': -0.0, 'AveOccup': -0.0, 'Latitude': -0.42, 'Longitude': -0.43}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6062326851971466"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Ridge(alpha=1e-2)\n",
    "model.fit(data,target)\n",
    "print(dict(list(zip(model.feature_names_in_,np.round(model.coef_,2)))))\n",
    "model.score(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79651175-bb3f-44ac-a619-c9fa3e16277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MedInc': 0.39, 'HouseAge': 0.02, 'AveRooms': -0.0, 'AveBedrms': 0.0, 'Population': 0.0, 'AveOccup': -0.0, 'Latitude': -0.11, 'Longitude': -0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5452665703368436"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Lasso(alpha=0.1)\n",
    "model.fit(data,target)\n",
    "print(dict(list(zip(model.feature_names_in_,np.round(model.coef_,2)))))\n",
    "model.score(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c1d7c6-1c27-48a9-9df9-5a598539d605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MedInc': 0.34, 'HouseAge': 0.01, 'AveRooms': 0.0, 'AveBedrms': 0.0, 'Population': 0.0, 'AveOccup': -0.0, 'Latitude': -0.09, 'Longitude': -0.07}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5216354749756258"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ElasticNet(alpha=0.7, l1_ratio=0.1)\n",
    "model.fit(data,target)\n",
    "print(dict(list(zip(model.feature_names_in_,np.round(model.coef_,2)))))\n",
    "model.score(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f9c72a7-fe7a-485b-9ac9-04f855b0177a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.355e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.302e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.321e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.289e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.369e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.255e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.387e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.363e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.373e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.328e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.309e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.262e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.298e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.376e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.380e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.394e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.262e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.392e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.374e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.335e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.431e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.437e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.459e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.422e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.326e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.786e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.785e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.809e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.747e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.740e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.798e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.854e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.818e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.814e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.714e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.312e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.295e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.244e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.313e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.296e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.382e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.257e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.228e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.289e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.302e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.343e+03, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.311e+03, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.269e+03, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.074e+04, tolerance: 2.206e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.300e+03, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+04, tolerance: 2.182e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.255e+03, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.451e+03, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.255e+03, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.069e+04, tolerance: 2.196e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+04, tolerance: 2.184e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.290e+03, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.305e+03, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.066e+04, tolerance: 2.191e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.248e+03, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.069e+04, tolerance: 2.197e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.072e+04, tolerance: 2.204e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.088e+04, tolerance: 2.235e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.072e+04, tolerance: 2.205e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.065e+04, tolerance: 2.188e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:910: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/kaygun/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.411e+03, tolerance: 2.748e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV,RepeatedKFold\n",
    "from numpy import arange\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "grid = {}\n",
    "grid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "grid['l1_ratio'] = arange(0, 1, 0.1)\n",
    "\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "results = search.fit(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b639cd94-60e4-4c58-98d2-676e0b0eee7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=5, random_state=1),\n",
       "             estimator=ElasticNet(alpha=0.7, l1_ratio=0.1), n_jobs=-1,\n",
       "             param_grid={&#x27;alpha&#x27;: [1e-05, 0.0001, 0.001, 0.01, 0.1, 0.0, 1.0,\n",
       "                                   10.0, 100.0],\n",
       "                         &#x27;l1_ratio&#x27;: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=5, random_state=1),\n",
       "             estimator=ElasticNet(alpha=0.7, l1_ratio=0.1), n_jobs=-1,\n",
       "             param_grid={&#x27;alpha&#x27;: [1e-05, 0.0001, 0.001, 0.01, 0.1, 0.0, 1.0,\n",
       "                                   10.0, 100.0],\n",
       "                         &#x27;l1_ratio&#x27;: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(alpha=0.7, l1_ratio=0.1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(alpha=0.7, l1_ratio=0.1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=RepeatedKFold(n_repeats=2, n_splits=5, random_state=1),\n",
       "             estimator=ElasticNet(alpha=0.7, l1_ratio=0.1), n_jobs=-1,\n",
       "             param_grid={'alpha': [1e-05, 0.0001, 0.001, 0.01, 0.1, 0.0, 1.0,\n",
       "                                   10.0, 100.0],\n",
       "                         'l1_ratio': array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67281b78-3d9c-4d1d-be54-7f484955b119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0, 'l1_ratio': 0.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02b15f-0792-49f9-88d2-06ba84cecdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
